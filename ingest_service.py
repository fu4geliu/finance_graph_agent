import time
import json
import datetime
import os
import re
import torch
import akshare as ak
from openai import OpenAI
from dotenv import load_dotenv
from transformers import AutoModelForCausalLM, AutoTokenizer
from database import run_query

load_dotenv()

# âœ… äº‘ç«¯æ¨¡å‹é…ç½®ï¼šä½¿ç”¨ DashScope
cloud_client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url=os.getenv("DASHSCOPE_BASE_URL")
)
CLOUD_MODEL_NAME = os.getenv("LLM_MODEL", "qwen-max")

# âœ… æœ¬åœ°æ¨¡å‹é…ç½®
LOCAL_MODEL_PATH = os.getenv("LOCAL_MODEL_PATH", "")
COMPLEXITY_THRESHOLD = float(os.getenv("COMPLEXITY_THRESHOLD", "0.55"))  # å¤æ‚åº¦é˜ˆå€¼

# å…¨å±€æ¨¡å‹å˜é‡ï¼ˆæ‡’åŠ è½½ï¼‰
_local_tokenizer = None
_local_model = None

def get_realtime_news():
    """è·å–è´¢è”ç¤¾ç”µæŠ¥"""
    try:
        print("ğŸ“¡ æ­£åœ¨æ‹‰å–æœ€æ–°ç”µæŠ¥...")
        df = ak.stock_telegraph_cls(symbol="å…¨éƒ¨")
        if df.empty: return []
        return df.head(3)['content'].tolist()
    except Exception as e:
        print(f"âŒ è·å–æ–°é—»å¤±è´¥: {e}")
        return []

def score_news_complexity(news_text):
    """æ ¹æ®é•¿åº¦ã€å¥å­æ•°ã€å®ä½“/æ•°å­—æ•°æ‰“åˆ†ï¼Œç”¨äºåç»­è·¯ç”±å†³ç­–ã€‚"""
    if not news_text:
        return 0.0
    length_score = min(len(news_text) / 600, 1.2)
    sentence_score = min(len(re.split(r"[ã€‚ï¼ï¼Ÿ!?]", news_text)) / 8, 1.0)
    entity_matches = re.findall(r"[A-Za-z0-9\u4e00-\u9fa5]{2,}", news_text)
    unique_entities = len(set(entity_matches))
    entity_score = min(unique_entities / 50, 1.0)
    digit_score = min(len(re.findall(r"\d+", news_text)) / 10, 1.0)
    score = (0.4 * length_score) + (0.25 * sentence_score) + (0.25 * entity_score) + (0.1 * digit_score)
    return round(min(score, 1.0), 3)

def parse_events_from_text(text):
    """å°è¯•ä»æ¨¡å‹è¾“å‡ºä¸­è§£æ JSON åˆ—è¡¨ã€‚"""
    if not text:
        return []
    json_block = re.search(r"```json\s*(.*?)\s*```", text, re.DOTALL)
    candidate = json_block.group(1) if json_block else text
    try:
        events = json.loads(candidate)
        if isinstance(events, list):
            return events
    except json.JSONDecodeError:
        pass
    return []

def events_valid(events):
    """ç®€å•æ ¡éªŒäº‹ä»¶åˆ—è¡¨æ˜¯å¦æ»¡è¶³æœ€åŸºæœ¬ç»“æ„ã€‚"""
    if not isinstance(events, list) or not events:
        return False
    for event in events:
        if not isinstance(event, dict):
            return False
        if "event_type" not in event or "trigger" not in event or "arguments" not in event:
            return False
    return True

def initialize_local_model():
    """åˆå§‹åŒ–æœ¬åœ°å¾®è°ƒæ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰ã€‚"""
    global _local_tokenizer, _local_model
    
    if _local_tokenizer is not None and _local_model is not None:
        return _local_tokenizer, _local_model
    
    if not LOCAL_MODEL_PATH or not os.path.exists(LOCAL_MODEL_PATH):
        print("âš ï¸ æœ¬åœ°æ¨¡å‹è·¯å¾„æœªé…ç½®æˆ–ä¸å­˜åœ¨ï¼Œå°†åªä½¿ç”¨äº‘ç«¯æ¨¡å‹")
        return None, None
    
    try:
        print("ğŸ”„ æ­£åœ¨åŠ è½½æœ¬åœ°å¾®è°ƒæ¨¡å‹...")
        _local_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)
        _local_model = AutoModelForCausalLM.from_pretrained(
            LOCAL_MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_4bit=False,
            use_safetensors=True
        )
        print("âœ… æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼")
        return _local_tokenizer, _local_model
    except Exception as e:
        print(f"âŒ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°†åªä½¿ç”¨äº‘ç«¯æ¨¡å‹")
        return None, None

def extract_with_local_model(news_text, tokenizer, model):
    """ä½¿ç”¨æœ¬åœ°å¾®è°ƒæ¨¡å‹æŠ½å–äº‹ä»¶ã€‚"""
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªé‡‘èæ–°é—»ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚è¯·åªè¾“å‡º JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
- "event_type"
- "trigger"
- "arguments": åŒ…å«"ä¸»ä½“"ã€"å®¢ä½“"ã€"æ—¶é—´"ã€"åœ°ç‚¹"ç­‰é”®ï¼Œç¼ºå¤±å­—æ®µå¯çœç•¥ã€‚
æ–°é—»å†…å®¹ï¼š
{news_text}

ä¸¥æ ¼è¾“å‡º JSONï¼ˆå‹¿è¾“å‡ºå…¶å®ƒè§£é‡Šï¼‰ã€‚
""".strip()
    try:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        )
        if isinstance(inputs, torch.Tensor):
            inputs = {"input_ids": inputs}
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        if "attention_mask" not in inputs:
            inputs["attention_mask"] = torch.ones_like(inputs["input_ids"])
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=False,
                temperature=0.2,
                top_p=0.9
            )
        response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
        events = parse_events_from_text(response.strip())
        if events_valid(events):
            print("   âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹æŠ½å–æˆåŠŸ")
            return events
        print("   âš ï¸ æœ¬åœ°æ¨¡å‹è¾“å‡ºæœªé€šè¿‡æ ¡éªŒï¼Œå‡†å¤‡å‡çº§ä¸ºäº‘ç«¯æ¨¡å‹")
        return []
    except Exception as e:
        print(f"   âš ï¸ æœ¬åœ°æ¨¡å‹æŠ½å–å¼‚å¸¸: {e}ï¼Œåˆ‡æ¢åˆ°äº‘ç«¯æ¨¡å‹")
        return []

def extract_with_cloud_model(news_text):
    """ä½¿ç”¨äº‘ç«¯å¤§æ¨¡å‹ï¼ˆDashScopeï¼‰æŠ½å–äº‹ä»¶ã€‚"""
    prompt = f"""
ä»ä»¥ä¸‹æ–°é—»ä¸­æå–å…³é”®é‡‘èäº‹ä»¶ã€‚è¿”å›JSONåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
- event_type (å¦‚: æŠ•èµ„, æ¶¨åœ, ç½šæ¬¾, æ”¶è´­)
- trigger (è§¦å‘è¯)
- arguments (å­—å…¸, åŒ…å«: ä¸»ä½“, å®¢ä½“, é‡‘é¢, åŸå› ç­‰)

æ–°é—»: {news_text}

æ³¨æ„ï¼šè¯·ç›´æ¥è¿”å›çº¯ JSON æ•°ç»„ï¼Œä¸è¦åŒ…å« Markdown æ ¼å¼ï¼ˆå¦‚ ```json ... ```ï¼‰ã€‚
"""
    try:
        response = cloud_client.chat.completions.create(
            model=CLOUD_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1
        )
        content = response.choices[0].message.content
        # æ¸…æ´—å¯èƒ½å­˜åœ¨çš„ Markdown æ ‡è®°
        content = content.replace("```json", "").replace("```", "").strip()
        events = parse_events_from_text(content)
        if events_valid(events):
            print("   âœ… ä½¿ç”¨äº‘ç«¯æ¨¡å‹æŠ½å–æˆåŠŸ")
            return events
        print("   âŒ äº‘ç«¯æ¨¡å‹å“åº”æœªè§£æå‡ºæœ‰æ•ˆ JSON")
        return []
    except json.JSONDecodeError:
        print(f"   âš ï¸ JSON è§£æå¤±è´¥: {content[:50] if 'content' in locals() else 'unknown'}...")
        return []
    except Exception as e:
        print(f"   âŒ äº‘ç«¯æ¨¡å‹æŠ½å–å¤±è´¥: {e}")
        return []

def extract_events(text):
    """æ ¹æ®å¤æ‚åº¦åœ¨æœ¬åœ°æ¨¡å‹ä¸äº‘ç«¯æ¨¡å‹ä¹‹é—´è·¯ç”±ã€‚"""
    complexity = score_news_complexity(text)
    print(f"   ğŸ“Š æ–‡æœ¬å¤æ‚åº¦å¾—åˆ†: {complexity:.3f} (é˜ˆå€¼: {COMPLEXITY_THRESHOLD})")
    
    # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹å¤„ç†ç®€å•æ–°é—»
    tokenizer, model = initialize_local_model()
    prefer_cloud = complexity >= COMPLEXITY_THRESHOLD
    
    if not prefer_cloud and tokenizer is not None and model is not None:
        events = extract_with_local_model(text, tokenizer, model)
        if events_valid(events):
            return events
        print("   â« æœ¬åœ°æ¨¡å‹å¤„ç†å¤±è´¥ï¼Œåˆ‡æ¢åˆ°äº‘ç«¯æ¨¡å‹...")
    
    # å¤æ‚æ–°é—»æˆ–æœ¬åœ°æ¨¡å‹å¤±è´¥æ—¶ä½¿ç”¨äº‘ç«¯æ¨¡å‹
    events = extract_with_cloud_model(text)
    if events_valid(events):
        return events
    
    print("   âŒ äº‹ä»¶æå–å…¨éƒ¨å¤±è´¥")
    return []

def save_to_neo4j(events):
    """å­˜å…¥å›¾æ•°æ®åº“ (ä¿æŒä¸å˜)"""
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    for event in events:
        e_type = event.get("event_type", "æœªçŸ¥")
        trigger = event.get("trigger", "æœªçŸ¥")
        args = event.get("arguments", {})
        
        cypher_event = """
        MERGE (e:Event {type: $type, trigger: $trigger})
        ON CREATE SET e.timestamp = $time, e.created_at = timestamp()
        ON MATCH SET e.last_seen = $time
        RETURN elementId(e) as id
        """
        run_query(cypher_event, {"type": e_type, "trigger": trigger, "time": current_time})
        
        for role, name in args.items():
            if not name or not isinstance(name, str): continue
            # ç®€å•çš„å®ä½“åæ¸…æ´—
            name = name.replace('"', '').replace("'", "")
            
            cypher_rel = """
            MATCH (e:Event {type: $type, trigger: $trigger})
            MERGE (ent:Entity {name: $name})
            MERGE (ent)-[:PARTICIPATES_IN {role: $role}]->(e)
            """
            run_query(cypher_rel, {"type": e_type, "trigger": trigger, "name": name, "role": role})
            print(f"   âœ… å…³ç³»å…¥åº“: ({name})--[{role}]-->({e_type})")

def main_loop():
    processed_hashes = set()
    model_info = f"äº‘ç«¯: {CLOUD_MODEL_NAME}"
    if LOCAL_MODEL_PATH and os.path.exists(LOCAL_MODEL_PATH):
        model_info += f" | æœ¬åœ°: {os.path.basename(LOCAL_MODEL_PATH)}"
    print(f"ğŸš€ åå°é‡‡é›†æœåŠ¡å·²å¯åŠ¨ ({model_info})...")
    print(f"ğŸ“Œ å¤æ‚åº¦é˜ˆå€¼: {COMPLEXITY_THRESHOLD} (>=é˜ˆå€¼ä½¿ç”¨äº‘ç«¯æ¨¡å‹)")
    
    # é¢„åŠ è½½æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    if LOCAL_MODEL_PATH:
        initialize_local_model()
    
    while True:
        news_list = get_realtime_news()
        for news in news_list:
            h = hash(news)
            if h in processed_hashes: continue
            
            print(f"\nğŸ“° å¤„ç†æ–°é—»: {news[:30]}...")
            events = extract_events(news)
            if events:
                save_to_neo4j(events)
            processed_hashes.add(h)
        
        print("ğŸ’¤ ç­‰å¾… 60 ç§’...")
        time.sleep(60)

if __name__ == "__main__":
    main_loop()